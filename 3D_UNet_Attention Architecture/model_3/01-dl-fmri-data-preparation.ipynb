{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc68258e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-23T13:37:36.573277Z",
     "iopub.status.busy": "2025-06-23T13:37:36.572970Z",
     "iopub.status.idle": "2025-06-23T13:43:24.128576Z",
     "shell.execute_reply": "2025-06-23T13:43:24.125856Z"
    },
    "papermill": {
     "duration": 347.562381,
     "end_time": "2025-06-23T13:43:24.132174",
     "exception": false,
     "start_time": "2025-06-23T13:37:36.569793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTEBOOK 1A: Downloading Chunk 1 (Files 1-20)\n",
      "Streaming: sub-01_task-training_run-01_bold.nii.gz\n",
      "Streaming: sub-01_task-training_run-02_bold.nii.gz\n",
      "Streaming: sub-01_task-training_run-03_bold.nii.gz\n",
      "Streaming: sub-01_task-training_run-04_bold.nii.gz\n",
      "Streaming: sub-01_task-training_run-05_bold.nii.gz\n",
      "Streaming: sub-02_task-training_run-01_bold.nii.gz\n",
      "Streaming: sub-02_task-training_run-02_bold.nii.gz\n",
      "Streaming: sub-02_task-training_run-03_bold.nii.gz\n",
      "Streaming: sub-02_task-training_run-04_bold.nii.gz\n",
      "Streaming: sub-02_task-training_run-05_bold.nii.gz\n",
      "Streaming: sub-03_task-training_run-01_bold.nii.gz\n",
      "Streaming: sub-03_task-training_run-02_bold.nii.gz\n",
      "Streaming: sub-03_task-training_run-03_bold.nii.gz\n",
      "Streaming: sub-03_task-training_run-04_bold.nii.gz\n",
      "Streaming: sub-03_task-training_run-05_bold.nii.gz\n",
      "Streaming: sub-04_task-training_run-01_bold.nii.gz\n",
      "Streaming: sub-04_task-training_run-02_bold.nii.gz\n",
      "Streaming: sub-04_task-training_run-03_bold.nii.gz\n",
      "Streaming: sub-04_task-training_run-04_bold.nii.gz\n",
      "Streaming: sub-04_task-training_run-05_bold.nii.gz\n",
      "\n",
      "Chunk 1 saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import os, time, requests, tempfile, numpy as np, nibabel as nib, gc\n",
    "from typing import List, Optional\n",
    "\n",
    "class OpenNeuroStreamer:\n",
    "    # ... (sınıfın tam kodu) ...\n",
    "    def __init__(self, dataset_id: str = \"ds002306\"): self.dataset_id = dataset_id; self.base_url = f\"https://s3.amazonaws.com/openneuro.org/{dataset_id}\"\n",
    "    def check_url_exists(self, url: str) -> bool:\n",
    "        try: response = requests.head(url, timeout=15); return response.status_code == 200\n",
    "        except: return False\n",
    "    def stream_nii_file(self, url: str) -> Optional[np.ndarray]:\n",
    "        print(f\"Streaming: {os.path.basename(url)}\"); temp_path = None\n",
    "        try:\n",
    "            response = requests.get(url, stream=True, timeout=120); response.raise_for_status()\n",
    "            with tempfile.NamedTemporaryFile(suffix='.nii.gz', delete=False) as temp_file:\n",
    "                temp_path = temp_file.name\n",
    "                for chunk in response.iter_content(chunk_size=8192): temp_file.write(chunk)\n",
    "            img = nib.load(temp_path); return img.get_fdata(dtype=np.float32)\n",
    "        except Exception as e: print(f\"Error streaming file: {e}\"); return None\n",
    "        finally:\n",
    "            if temp_path and os.path.exists(temp_path): os.unlink(temp_path)\n",
    "    def load_files_for_chunk(self, subjects, num_files, urls):\n",
    "        data = [];\n",
    "        for s in subjects:\n",
    "            if len(data) >= num_files: break\n",
    "            for r in range(1, 6):\n",
    "                if len(data) >= num_files: break\n",
    "                url = f\"{self.base_url}/{s}/func/{s}_task-training_run-{r:02d}_bold.nii.gz\"\n",
    "                if url in urls: continue\n",
    "                if self.check_url_exists(url):\n",
    "                    d = self.stream_nii_file(url)\n",
    "                    if d is not None: data.append(d); urls.add(url)\n",
    "        return data, urls\n",
    "\n",
    "# --- MAIN ---\n",
    "print(\"NOTEBOOK 1A: Downloading Chunk 1 (Files 1-20)\")\n",
    "streamer = OpenNeuroStreamer()\n",
    "subjects_pool = [f\"sub-{i:02d}\" for i in range(1, 11)]\n",
    "chunk_data, _ = streamer.load_files_for_chunk(subjects_pool, 20, set())\n",
    "if chunk_data:\n",
    "    np.save(\"/kaggle/working/fmri_dataset_chunk_1.npy\", np.array(chunk_data, dtype=np.float32))\n",
    "    print(\"\\nChunk 1 saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 353.04037,
   "end_time": "2025-06-23T13:43:24.777204",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-23T13:37:31.736834",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
